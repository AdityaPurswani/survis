@article{WU2011768,
title = {Automatic speech emotion recognition using modulation spectral features},
journal = {Speech Communication},
volume = {53},
number = {5},
pages = {768-785},
year = {2011},
note = {Perceptual and Statistical Audition},
issn = {0167-6393},
doi = {https://doi.org/10.1016/j.specom.2010.08.013},
author = {Siqing Wu and Tiago H. Falk and Wai-Yip Chan},
keywords = {Emotion recognition, Speech modulation, Spectro-temporal representation, Affective computing, Speech analysis},
abstract = {In this study, modulation spectral features (MSFs) are proposed for the automatic recognition of human affective information from speech. The features are extracted from an auditory-inspired long-term spectro-temporal representation. Obtained using an auditory filterbank and a modulation filterbank for speech analysis, the representation captures both acoustic frequency and temporal modulation frequency components, thereby conveying information that is important for human speech perception but missing from conventional short-term spectral features. On an experiment assessing classification of discrete emotion categories, the MSFs show promising performance in comparison with features that are based on mel-frequency cepstral coefficients and perceptual linear prediction coefficients, two commonly used short-term spectral representations. The MSFs further render a substantial improvement in recognition performance when used to augment prosodic features, which have been extensively used for emotion recognition. Using both types of features, an overall recognition rate of 91.6% is obtained for classifying seven emotion categories. Moreover, in an experiment assessing recognition of continuous emotions, the proposed features in combination with prosodic features attain estimation performance comparable to human evaluation.}
},
@article{SCHERER2003227,
title = {Vocal communication of emotion: A review of research paradigms},
journal = {Speech Communication},
volume = {40},
number = {1},
pages = {227-256},
year = {2003},
issn = {0167-6393},
doi = {https://doi.org/10.1016/S0167-6393(02)00084-5},
author = {Klaus R Scherer},
keywords = {Vocal communication, Expression of emotion, Speaker moods and attitudes, Speech technology, Theories of emotion, Evaluation of emotion effects on voice and speech, Acoustic markers of emotion, Emotion induction, Emotion simulation, Stress effects on voice, Perception/decoding},
abstract = {The current state of research on emotion effects on voice and speech is reviewed and issues for future research efforts are discussed. In particular, it is suggested to use the Brunswikian lens model as a base for research on the vocal communication of emotion. This approach allows one to model the complete process, including both encoding (expression), transmission, and decoding (impression) of vocal emotion communication. Special emphasis is placed on the conceptualization and operationalization of the major elements of the model (i.e., the speaker’s emotional state, the listener’s attribution, and the mediating acoustic cues). In addition, the advantages and disadvantages of research paradigms for the induction or observation of emotional expression in voice and speech and the experimental manipulation of vocal cues are discussed, using pertinent examples drawn from past and present research.
Zusammenfassung
Der Aufsatz gibt einen umfassenden Überblick über den Forschungsstand zum Thema der Beeinflussung von Stimme und Sprechweise durch Emotionen des Sprechers. Allgemein wird vorgeschlagen, die Forschung zur vokalen Kommunikation der Emotionen am Brunswik’schen Linsenmodell zu orientieren. Dieser Ansatz erlaubt den gesamten Kommunikationsprozess zu modellieren, von der Enkodierung (Ausdruck), über die Transmission (Übertragung), bis zur Dekodierung (Eindruck). Besondere Aufmerksamkeit gilt den Problemen der Konzeptualisierung und Operationalisierung der zentralen Elemente des Modells (z.B., dem Emotionszustand des Sprechers, den Inferenzprozessen des Hörers, und den zugrundeliegenden vokalen Hinweisreizen). Anhand ausgewählter Beispiele empirischer Untersuchungen werden die Vor- und Nachteile verschiedener Forschungsparadigmen zur Induktion und Beobachtung des emotionalen Stimmausdrucks sowie zur experimentellen Manipulation vokaler Hinweisreize diskutiert.
Résumé
L’état actuel de la recherche sur l’effet des émotions d’un locuteur sur la voix et la parole est décrit et des approches prometteuses pour le futur identifiées. En particulier, le modèle de perception de Brunswik (dit “de la lentille” est proposé) comme paradigme pour la recherche sur la communication vocale des émotions. Ce modèle permet la modélisation du processus complet, de l’encodage (expression) par la transmission au décodage (impression). La conceptualisation et l’opérationalization des éléments centraux du modèle (l’état émotionnel du locuteur, l’inférence de cet état par l’auditeur, et les indices auditifs) sont discuté en détail. De plus, en analysant des exemples de la recherche dans le domaine, les avantages et désavantages de différentes méthodes pour l’induction et l’observation de l’expression émotionnelle dans la voix et la parole et pour la manipulation expérimentale de différents indices vocaux sont évoqués.}
},
@article{Murray1993TowardTS,
  title={Toward the simulation of emotion in synthetic speech: a review of the literature on human vocal emotion.},
  author={Iain R. Murray and John L. Arnott},
  journal={The Journal of the Acoustical Society of America},
  year={1993},
  doi={10.1121/1.405558},
  volume={93 2},
  pages={
          1097-108
        },
  keywords={Speech communication, Acoustics, Pitch, Speech analysis, voice quality}
  
},
@ARTICLE{9814838,
  author={Chen, Sanyuan and Wang, Chengyi and Chen, Zhengyang and Wu, Yu and Liu, Shujie and Chen, Zhuo and Li, Jinyu and Kanda, Naoyuki and Yoshioka, Takuya and Xiao, Xiong and Wu, Jian and Zhou, Long and Ren, Shuo and Qian, Yanmin and Qian, Yao and Wu, Jian and Zeng, Michael and Yu, Xiangzhan and Wei, Furu},
  journal={IEEE Journal of Selected Topics in Signal Processing}, 
  title={WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing}, 
  year={2022},
  volume={16},
  doi={10.1109/JSTSP.2022.3188113},
  number={6},
  pages={1505-1518},
  keywords={Predictive models,Self-supervised learning,Speech processing,Speech recognition,Convolution,Benchmark testing,Self-supervised learning,speech pre-training},
  },
@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    keywords={Natural Language Processing (NLP), Language Model Pre-training, Deep Learning, Transformers, Multimodal Emotion Recognition, Speech, Video, Text Data, Motion Capture, Dataset Limitations, Deep Learning Models Limitations},
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    doi = {10.18653/v1/N19-1423},
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
},
@INPROCEEDINGS{7477625,
  author={Jaiswal, Shashank and Valstar, Michel},
  booktitle={2016 IEEE Winter Conference on Applications of Computer Vision (WACV)}, 
  title={Deep learning the dynamic appearance and shape of facial action units}, 
  year={2016},
  volume={},
  number={},
  doi={10.1109/WACV.2016.7477625},
  pages={1-8},
  keywords={Shape,Gold,Face recognition,Machine learning,Feature extraction,Computer architecture,Neural networks},
  
},
@ARTICLE{9316154,
  author={Zhang, Yong and Cheng, Cheng and Zhang, Yidie},
  journal={IEEE Access}, 
  title={Multimodal Emotion Recognition Using a Hierarchical Fusion Convolutional Neural Network}, 
  year={2021},
  volume={9},
  doi={10.1109/ACCESS.2021.3049516},
  number={},
  pages={7943-7951},
  keywords={Feature extraction,Emotion recognition,Brain modeling,Electroencephalography,Physiology,Deep learning,Data models,Deep learning,electroencephalogram,hierarchical convolutional neural network,multimodal emotion recognition,multiscale features},
  },
@article{PORIA2015104,
title = {Towards an intelligent framework for multimodal affective data analysis},
journal = {Neural Networks},
volume = {63},
pages = {104-116},
year = {2015},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2014.10.005},
author = {Soujanya Poria and Erik Cambria and Amir Hussain and Guang-Bin Huang},
keywords = {Multimodal, Multimodal sentiment analysis, Facial expressions, Speech, Text, Emotion analysis, Affective computing},
abstract = {An increasingly large amount of multimodal content is posted on social media websites such as YouTube and Facebook everyday. In order to cope with the growth of such so much multimodal data, there is an urgent need to develop an intelligent multi-modal analysis framework that can effectively extract information from multiple modalities. In this paper, we propose a novel multimodal information extraction agent, which infers and aggregates the semantic and affective information associated with user-generated multimodal data in contexts such as e-learning, e-health, automatic video content tagging and human–computer interaction. In particular, the developed intelligent agent adopts an ensemble feature extraction approach by exploiting the joint use of tri-modal (text, audio and video) features to enhance the multimodal information extraction process. In preliminary experiments using the eNTERFACE dataset, our proposed multi-modal system is shown to achieve an accuracy of 87.95%, outperforming the best state-of-the-art system by more than 10%, or in relative terms, a 56% reduction in error rate.}
},
@article{PORIA201650,
title = {Fusing audio, visual and textual clues for sentiment analysis from multimodal content},
journal = {Neurocomputing},
volume = {174},
pages = {50-59},
year = {2016},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2015.01.095},
author = {Soujanya Poria and Erik Cambria and Newton Howard and Guang-Bin Huang and Amir Hussain},
keywords = {Multimodal fusion, Big social data analysis, Opinion mining, Multimodal sentiment analysis, Sentic computing},
abstract = {A huge number of videos are posted every day on social media platforms such as Facebook and YouTube. This makes the Internet an unlimited source of information. In the coming decades, coping with such information and mining useful knowledge from it will be an increasingly difficult task. In this paper, we propose a novel methodology for multimodal sentiment analysis, which consists in harvesting sentiments from Web videos by demonstrating a model that uses audio, visual and textual modalities as sources of information. We used both feature- and decision-level fusion methods to merge affective information extracted from multiple modalities. A thorough comparison with existing works in this area is carried out throughout the paper, which demonstrates the novelty of our approach. Preliminary comparative experiments with the YouTube dataset show that the proposed multimodal system achieves an accuracy of nearly 80%, outperforming all state-of-the-art systems by more than 20%.}
},
@article{Zadeh2018MultiattentionRN,
  title={Multi-attention Recurrent Network for Human Communication Comprehension},
  author={Amir Zadeh and Paul Pu Liang and Soujanya Poria and Prateek Vij and E. Cambria and Louis-Philippe Morency},
  journal={Proceedings of the ... AAAI Conference on Artificial Intelligence. AAAI Conference on Artificial Intelligence},
  year={2018},
  doi={10.1609/aaai.v32i1.12024},
  volume={2018},
  pages={
          5642-5649
        },
  keywords = {Multimodal Machine Learning, Attention Networks, Attention Modeling, Natural Language Processing}
  
},
@article{lian2023survey,
  title={A Survey of Deep Learning-Based Multimodal Emotion Recognition: Speech, Text, and Face},
  author={Lian, Hailun and Lu, Cheng and Li, Sunan and Zhao, Yan and Tang, Chuangao and Zong, Yuan},
  journal={Entropy},
  volume={25},
  doi = {10.3390/e25101440},
  number={10},
  pages={1440},
  year={2023},
  publisher={MDPI},
  keywords={deep learning, fusion method, multimodal emotion recognition, survey}
},
@ARTICLE{6487473,
  author={Wöllmer, Martin and Weninger, Felix and Knaup, Tobias and Schuller, Björn and Sun, Congkai and Sagae, Kenji and Morency, Louis-Philippe},
  journal={IEEE Intelligent Systems}, 
  title={YouTube Movie Reviews: Sentiment Analysis in an Audio-Visual Context}, 
  year={2013},
  doi={10.1109/MIS.2013.34},
  volume={28},
  number={3},
  pages={46-53},
  keywords={Videos,Motion pictures,Pragmatics,Context awareness,Feature extraction,YouTube,Visualization,Videos,Motion pictures,Pragmatics,Context awareness,Feature extraction,YouTube,Visualization,intelligent systems,sentiment analysis,affective computing,audio-visual pattern recognition,linguistic analysis},
  },
@article{article,
author = {Busso, Carlos and Bulut, Murtaza and Lee, Chi-Chun and Kazemzadeh, Abe and Mower Provost, Emily and Kim, Samuel and Chang, Jeannette and Lee, Sungbok and Narayanan, Shrikanth},
year = {2008},
month = {12},
doi = {10.1007/s10579-008-9076-6},
pages = {335-359},
title = {IEMOCAP: Interactive emotional dyadic motion capture database},
volume = {42},
journal = {Language Resources and Evaluation},
keywords={Audio-visual database, Dyadic interaction, Emotion, Emotional assessment, Motion capture system}
},
@article{Kossaifi_2021,
   title={SEWA DB: A Rich Database for Audio-Visual Emotion and Sentiment Research in the Wild},
   volume={43},
   ISSN={1939-3539},
   DOI={10.1109/tpami.2019.2944808},
   number={3},
   journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
   publisher={Institute of Electrical and Electronics Engineers (IEEE)},
   author={Kossaifi, Jean and Walecki, Robert and Panagakis, Yannis and Shen, Jie and Schmitt, Maximilian and Ringeval, Fabien and Han, Jing and Pandit, Vedhas and Toisoul, Antoine and Schuller, Bjorn and Star, Kam and Hajiyev, Elnar and Pantic, Maja},
   year={2021},
   month=mar, pages={1022–1040}, 
   keywords={SEWA, affect analysis, in-the-wild, emotion recognition, database, valence, arousal, facial action units}
},