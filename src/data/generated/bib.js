const generatedBibEntries = {
    "6487473": {
        "author": "W\u00f6llmer, Martin and Weninger, Felix and Knaup, Tobias and Schuller, Bj\u00f6rn and Sun, Congkai and Sagae, Kenji and Morency, Louis-Philippe",
        "doi": "10.1109/MIS.2013.34",
        "journal": "IEEE Intelligent Systems",
        "keywords": "Videos,Motion pictures,Pragmatics,Context awareness,Feature extraction,YouTube,Visualization,Videos,Motion pictures,Pragmatics,Context awareness,Feature extraction,YouTube,Visualization,intelligent systems,sentiment analysis,affective computing,audio-visual pattern recognition,linguistic analysis ,",
        "number": "3",
        "pages": "46-53",
        "title": "YouTube Movie Reviews: Sentiment Analysis in an Audio-Visual Context",
        "type": "ARTICLE",
        "volume": "28",
        "year": "2013"
    },
    "7477625": {
        "author": "Jaiswal, Shashank and Valstar, Michel",
        "booktitle": "2016 IEEE Winter Conference on Applications of Computer Vision (WACV)",
        "doi": "10.1109/WACV.2016.7477625",
        "keywords": "Shape,Gold,Face recognition,Machine learning,Feature extraction,Computer architecture,Neural networks ,",
        "number": "",
        "pages": "1-8",
        "title": "Deep learning the dynamic appearance and shape of facial action units",
        "type": "INPROCEEDINGS",
        "volume": "",
        "year": "2016"
    },
    "9316154": {
        "author": "Zhang, Yong and Cheng, Cheng and Zhang, Yidie",
        "doi": "10.1109/ACCESS.2021.3049516",
        "journal": "IEEE Access",
        "keywords": "Feature extraction,Emotion recognition,Brain modeling,Electroencephalography,Physiology,Deep learning,Data models,Deep learning,electroencephalogram,hierarchical convolutional neural network,multimodal emotion recognition,multiscale features ,",
        "number": "",
        "pages": "7943-7951",
        "title": "Multimodal Emotion Recognition Using a Hierarchical Fusion Convolutional Neural Network",
        "type": "ARTICLE",
        "volume": "9",
        "year": "2021"
    },
    "9814838": {
        "author": "Chen, Sanyuan and Wang, Chengyi and Chen, Zhengyang and Wu, Yu and Liu, Shujie and Chen, Zhuo and Li, Jinyu and Kanda, Naoyuki and Yoshioka, Takuya and Xiao, Xiong and Wu, Jian and Zhou, Long and Ren, Shuo and Qian, Yanmin and Qian, Yao and Wu, Jian and Zeng, Michael and Yu, Xiangzhan and Wei, Furu",
        "doi": "10.1109/JSTSP.2022.3188113",
        "journal": "IEEE Journal of Selected Topics in Signal Processing",
        "keywords": "Predictive models,Self-supervised learning,Speech processing,Speech recognition,Convolution,Benchmark testing,Self-supervised learning,speech pre-training ,",
        "number": "6",
        "pages": "1505-1518",
        "title": "WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing",
        "type": "ARTICLE",
        "volume": "16",
        "year": "2022"
    },
    "Kossaifi_2021": {
        "author": "Kossaifi, Jean and Walecki, Robert and Panagakis, Yannis and Shen, Jie and Schmitt, Maximilian and Ringeval, Fabien and Han, Jing and Pandit, Vedhas and Toisoul, Antoine and Schuller, Bjorn and Star, Kam and Hajiyev, Elnar and Pantic, Maja",
        "doi": "10.1109/tpami.2019.2944808",
        "issn": "1939-3539",
        "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
        "keywords": "SEWA, affect analysis, in-the-wild, emotion recognition, database, valence, arousal, facial action units ,",
        "month": "mar, pages",
        "number": "3",
        "publisher": "Institute of Electrical and Electronics Engineers (IEEE)",
        "title": "SEWA DB: A Rich Database for Audio-Visual Emotion and Sentiment Research in the Wild",
        "type": "article",
        "volume": "43",
        "year": "2021"
    },
    "Murray1993TowardTS": {
        "author": "Iain R. Murray and John L. Arnott",
        "doi": "10.1121/1.405558",
        "journal": "The Journal of the Acoustical Society of America",
        "keywords": "Speech communication, Acoustics, Pitch, Speech analysis, voice quality ,",
        "pages": " 1097-108 ,",
        "title": "Toward the simulation of emotion in synthetic speech: a review of the literature on human vocal emotion.",
        "type": "article",
        "volume": "93 2",
        "year": "1993"
    },
    "PORIA2015104": {
        "abstract": "An increasingly large amount of multimodal content is posted on social media websites such as YouTube and Facebook everyday. In order to cope with the growth of such so much multimodal data, there is an urgent need to develop an intelligent multi-modal analysis framework that can effectively extract information from multiple modalities. In this paper, we propose a novel multimodal information extraction agent, which infers and aggregates the semantic and affective information associated with user-generated multimodal data in contexts such as e-learning, e-health, automatic video content tagging and human\u2013computer interaction. In particular, the developed intelligent agent adopts an ensemble feature extraction approach by exploiting the joint use of tri-modal (text, audio and video) features to enhance the multimodal information extraction process. In preliminary experiments using the eNTERFACE dataset, our proposed multi-modal system is shown to achieve an accuracy of 87.95%, outperforming the best state-of-the-art system by more than 10%, or in relative terms, a 56% reduction in error rate. ,",
        "author": "Soujanya Poria and Erik Cambria and Amir Hussain and Guang-Bin Huang",
        "doi": "https://doi.org/10.1016/j.neunet.2014.10.005",
        "issn": "0893-6080",
        "journal": "Neural Networks",
        "keywords": "Multimodal, Multimodal sentiment analysis, Facial expressions, Speech, Text, Emotion analysis, Affective computing",
        "pages": "104-116",
        "title": "Towards an intelligent framework for multimodal affective data analysis",
        "type": "article",
        "volume": "63",
        "year": "2015"
    },
    "PORIA201650": {
        "abstract": "A huge number of videos are posted every day on social media platforms such as Facebook and YouTube. This makes the Internet an unlimited source of information. In the coming decades, coping with such information and mining useful knowledge from it will be an increasingly difficult task. In this paper, we propose a novel methodology for multimodal sentiment analysis, which consists in harvesting sentiments from Web videos by demonstrating a model that uses audio, visual and textual modalities as sources of information. We used both feature- and decision-level fusion methods to merge affective information extracted from multiple modalities. A thorough comparison with existing works in this area is carried out throughout the paper, which demonstrates the novelty of our approach. Preliminary comparative experiments with the YouTube dataset show that the proposed multimodal system achieves an accuracy of nearly 80%, outperforming all state-of-the-art systems by more than 20%. ,",
        "author": "Soujanya Poria and Erik Cambria and Newton Howard and Guang-Bin Huang and Amir Hussain",
        "doi": "https://doi.org/10.1016/j.neucom.2015.01.095",
        "issn": "0925-2312",
        "journal": "Neurocomputing",
        "keywords": "Multimodal fusion, Big social data analysis, Opinion mining, Multimodal sentiment analysis, Sentic computing",
        "pages": "50-59",
        "title": "Fusing audio, visual and textual clues for sentiment analysis from multimodal content",
        "type": "article",
        "volume": "174",
        "year": "2016"
    },
    "SCHERER2003227": {
        "abstract": "The current state of research on emotion effects on voice and speech is reviewed and issues for future research efforts are discussed. In particular, it is suggested to use the Brunswikian lens model as a base for research on the vocal communication of emotion. This approach allows one to model the complete process, including both encoding (expression), transmission, and decoding (impression) of vocal emotion communication. Special emphasis is placed on the conceptualization and operationalization of the major elements of the model (i.e., the speaker\u2019s emotional state, the listener\u2019s attribution, and the mediating acoustic cues). In addition, the advantages and disadvantages of research paradigms for the induction or observation of emotional expression in voice and speech and the experimental manipulation of vocal cues are discussed, using pertinent examples drawn from past and present research. Zusammenfassung Der Aufsatz gibt einen umfassenden \u00dcberblick \u00fcber den Forschungsstand zum Thema der Beeinflussung von Stimme und Sprechweise durch Emotionen des Sprechers. Allgemein wird vorgeschlagen, die Forschung zur vokalen Kommunikation der Emotionen am Brunswik\u2019schen Linsenmodell zu orientieren. Dieser Ansatz erlaubt den gesamten Kommunikationsprozess zu modellieren, von der Enkodierung (Ausdruck), \u00fcber die Transmission (\u00dcbertragung), bis zur Dekodierung (Eindruck). Besondere Aufmerksamkeit gilt den Problemen der Konzeptualisierung und Operationalisierung der zentralen Elemente des Modells (z.B., dem Emotionszustand des Sprechers, den Inferenzprozessen des H\u00f6rers, und den zugrundeliegenden vokalen Hinweisreizen). Anhand ausgew\u00e4hlter Beispiele empirischer Untersuchungen werden die Vor- und Nachteile verschiedener Forschungsparadigmen zur Induktion und Beobachtung des emotionalen Stimmausdrucks sowie zur experimentellen Manipulation vokaler Hinweisreize diskutiert. R\u00e9sum\u00e9 L\u2019\u00e9tat actuel de la recherche sur l\u2019effet des \u00e9motions d\u2019un locuteur sur la voix et la parole est d\u00e9crit et des approches prometteuses pour le futur identifi\u00e9es. En particulier, le mod\u00e8le de perception de Brunswik (dit \u201cde la lentille\u201d est propos\u00e9) comme paradigme pour la recherche sur la communication vocale des \u00e9motions. Ce mod\u00e8le permet la mod\u00e9lisation du processus complet, de l\u2019encodage (expression) par la transmission au d\u00e9codage (impression). La conceptualisation et l\u2019op\u00e9rationalization des \u00e9l\u00e9ments centraux du mod\u00e8le (l\u2019\u00e9tat \u00e9motionnel du locuteur, l\u2019inf\u00e9rence de cet \u00e9tat par l\u2019auditeur, et les indices auditifs) sont discut\u00e9 en d\u00e9tail. De plus, en analysant des exemples de la recherche dans le domaine, les avantages et d\u00e9savantages de diff\u00e9rentes m\u00e9thodes pour l\u2019induction et l\u2019observation de l\u2019expression \u00e9motionnelle dans la voix et la parole et pour la manipulation exp\u00e9rimentale de diff\u00e9rents indices vocaux sont \u00e9voqu\u00e9s. ,",
        "author": "Klaus R Scherer",
        "doi": "https://doi.org/10.1016/S0167-6393(02)00084-5",
        "issn": "0167-6393",
        "journal": "Speech Communication",
        "keywords": "Vocal communication, Expression of emotion, Speaker moods and attitudes, Speech technology, Theories of emotion, Evaluation of emotion effects on voice and speech, Acoustic markers of emotion, Emotion induction, Emotion simulation, Stress effects on voice, Perception/decoding",
        "number": "1",
        "pages": "227-256",
        "title": "Vocal communication of emotion: A review of research paradigms",
        "type": "article",
        "volume": "40",
        "year": "2003"
    },
    "WU2011768": {
        "abstract": "In this study, modulation spectral features (MSFs) are proposed for the automatic recognition of human affective information from speech. The features are extracted from an auditory-inspired long-term spectro-temporal representation. Obtained using an auditory filterbank and a modulation filterbank for speech analysis, the representation captures both acoustic frequency and temporal modulation frequency components, thereby conveying information that is important for human speech perception but missing from conventional short-term spectral features. On an experiment assessing classification of discrete emotion categories, the MSFs show promising performance in comparison with features that are based on mel-frequency cepstral coefficients and perceptual linear prediction coefficients, two commonly used short-term spectral representations. The MSFs further render a substantial improvement in recognition performance when used to augment prosodic features, which have been extensively used for emotion recognition. Using both types of features, an overall recognition rate of 91.6% is obtained for classifying seven emotion categories. Moreover, in an experiment assessing recognition of continuous emotions, the proposed features in combination with prosodic features attain estimation performance comparable to human evaluation. ,",
        "author": "Siqing Wu and Tiago H. Falk and Wai-Yip Chan",
        "doi": "https://doi.org/10.1016/j.specom.2010.08.013",
        "issn": "0167-6393",
        "journal": "Speech Communication",
        "keywords": "Emotion recognition, Speech modulation, Spectro-temporal representation, Affective computing, Speech analysis",
        "note": "Perceptual and Statistical Audition",
        "number": "5",
        "pages": "768-785",
        "title": "Automatic speech emotion recognition using modulation spectral features",
        "type": "article",
        "volume": "53",
        "year": "2011"
    },
    "Zadeh2018MultiattentionRN": {
        "author": "Amir Zadeh and Paul Pu Liang and Soujanya Poria and Prateek Vij and E. Cambria and Louis-Philippe Morency",
        "doi": "10.1609/aaai.v32i1.12024",
        "journal": "Proceedings of the ... AAAI Conference on Artificial Intelligence. AAAI Conference on Artificial Intelligence",
        "keywords": "Multimodal Machine Learning, Attention Networks, Attention Modeling, Natural Language Processing ,",
        "pages": " 5642-5649 ,",
        "title": "Multi-attention Recurrent Network for Human Communication Comprehension",
        "type": "article",
        "volume": "2018",
        "year": "2018"
    },
    "article": {
        "author": "Busso, Carlos and Bulut, Murtaza and Lee, Chi-Chun and Kazemzadeh, Abe and Mower Provost, Emily and Kim, Samuel and Chang, Jeannette and Lee, Sungbok and Narayanan, Shrikanth",
        "doi": "10.1007/s10579-008-9076-6",
        "journal": "Language Resources and Evaluation",
        "keywords": "Audio-visual database, Dyadic interaction, Emotion, Emotional assessment, Motion capture system ,",
        "month": "12",
        "pages": "335-359",
        "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
        "type": "article",
        "volume": "42",
        "year": "2008"
    },
    "devlin-etal-2019-bert": {
        "abstract": "\"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\\%} (4.6{\\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\", ,",
        "address": "\"Minneapolis, Minnesota\",",
        "author": "\"Devlin, Jacob  and Chang, Ming-Wei  and Lee, Kenton  and Toutanova, Kristina\",",
        "booktitle": "\"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)\",",
        "doi": "10.18653/v1/N19-1423",
        "editor": "\"Burstein, Jill  and Doran, Christy  and Solorio, Thamar\",",
        "keywords": "Natural Language Processing (NLP), Language Model Pre-training, Deep Learning, Transformers, Multimodal Emotion Recognition, Speech, Video, Text Data, Motion Capture, Dataset Limitations, Deep Learning Models Limitations",
        "month": "jun,",
        "pages": "\"4171--4186\",",
        "publisher": "\"Association for Computational Linguistics\",",
        "title": "\"{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding\",",
        "type": "inproceedings",
        "year": "\"2019\","
    },
    "lian2023survey": {
        "author": "Lian, Hailun and Lu, Cheng and Li, Sunan and Zhao, Yan and Tang, Chuangao and Zong, Yuan",
        "doi": "10.3390/e25101440",
        "journal": "Entropy",
        "keywords": "deep learning, fusion method, multimodal emotion recognition, survey ,",
        "number": "10",
        "pages": "1440",
        "publisher": "MDPI",
        "title": "A Survey of Deep Learning-Based Multimodal Emotion Recognition: Speech, Text, and Face",
        "type": "article",
        "volume": "25",
        "year": "2023"
    }
};